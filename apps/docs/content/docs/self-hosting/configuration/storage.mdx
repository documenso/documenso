---
title: Storage Configuration
description: Configure file storage for uploaded documents and signed PDFs using database storage (default) or S3-compatible object storage.
---

import { Callout } from 'fumadocs-ui/components/callout';

## Storage Options

| Backend    | Best For                         | Scalability | Configuration |
| ---------- | -------------------------------- | ----------- | ------------- |
| `database` | Small deployments, simplicity    | Limited     | None required |
| `s3`       | Production, large files, backups | High        | Required      |

Select the storage backend with the `NEXT_PUBLIC_UPLOAD_TRANSPORT` environment variable:

```bash
# Database storage (default)
NEXT_PUBLIC_UPLOAD_TRANSPORT=database

# S3-compatible storage
NEXT_PUBLIC_UPLOAD_TRANSPORT=s3
```

---

## Database Storage

Database storage is the default option and requires no additional configuration. Documents are stored as base64-encoded data directly in PostgreSQL.

### Advantages

- No external dependencies
- Simple deployment
- Automatic backups with database

### Limitations

- Increases database size significantly
- Slower for large files
- Database backup/restore takes longer
- Not recommended for files larger than 10MB

### Configuration

No configuration required. Database storage is enabled when `NEXT_PUBLIC_UPLOAD_TRANSPORT` is unset or set to `database`.

---

## S3 Configuration

S3 storage is recommended for production deployments. Documenso supports AWS S3 and any S3-compatible storage service.

### Required Variables

| Variable                                | Description                       |
| --------------------------------------- | --------------------------------- |
| `NEXT_PUBLIC_UPLOAD_TRANSPORT`          | Set to `s3`                       |
| `NEXT_PRIVATE_UPLOAD_BUCKET`            | S3 bucket name                    |
| `NEXT_PRIVATE_UPLOAD_REGION`            | AWS region (default: `us-east-1`) |
| `NEXT_PRIVATE_UPLOAD_ACCESS_KEY_ID`     | AWS access key ID                 |
| `NEXT_PRIVATE_UPLOAD_SECRET_ACCESS_KEY` | AWS secret access key             |

### Optional Variables

| Variable                               | Description                                   | Default     |
| -------------------------------------- | --------------------------------------------- | ----------- |
| `NEXT_PRIVATE_UPLOAD_ENDPOINT`         | Custom S3 endpoint for S3-compatible services |             |
| `NEXT_PRIVATE_UPLOAD_FORCE_PATH_STYLE` | Use path-style URLs instead of virtual-hosted | `false`     |
| `NEXT_PRIVATE_UPLOAD_REGION`           | S3 region                                     | `us-east-1` |

---

## AWS S3 Setup

### 1. Create an S3 Bucket

Create a bucket in the AWS Console or using the CLI:

```bash
aws s3 mb s3://your-documenso-bucket --region us-east-1
```

### 2. Configure Bucket Policy

Block public access and configure CORS for presigned URL uploads:

**CORS Configuration:**

```json
[
  {
    "AllowedHeaders": ["*"],
    "AllowedMethods": ["GET", "PUT", "POST"],
    "AllowedOrigins": ["https://your-documenso-domain.com"],
    "ExposeHeaders": ["ETag"]
  }
]
```

Apply via AWS Console (Bucket > Permissions > CORS configuration) or CLI:

```bash
aws s3api put-bucket-cors --bucket your-documenso-bucket --cors-configuration file://cors.json
```

### 3. Create IAM User

Create an IAM user with programmatic access and attach this policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:PutObject", "s3:GetObject", "s3:DeleteObject"],
      "Resource": "arn:aws:s3:::your-documenso-bucket/*"
    }
  ]
}
```

### 4. Configure Environment Variables

```bash
NEXT_PUBLIC_UPLOAD_TRANSPORT=s3
NEXT_PRIVATE_UPLOAD_BUCKET=your-documenso-bucket
NEXT_PRIVATE_UPLOAD_REGION=us-east-1
NEXT_PRIVATE_UPLOAD_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
NEXT_PRIVATE_UPLOAD_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
```

---

## MinIO Setup

MinIO is a self-hosted S3-compatible object storage server.

### 1. Deploy MinIO

Using Docker:

```bash
docker run -d \
  --name minio \
  -p 9000:9000 \
  -p 9001:9001 \
  -e MINIO_ROOT_USER=minioadmin \
  -e MINIO_ROOT_PASSWORD=minioadmin \
  -v minio_data:/data \
  minio/minio server /data --console-address ":9001"
```

Using Docker Compose with Documenso:

```yaml
services:
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    ports:
      - '9000:9000'
      - '9001:9001'
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data

volumes:
  minio_data:
```

### 2. Create a Bucket

Access the MinIO Console at `http://localhost:9001` and create a bucket, or use the CLI:

```bash
# Install MinIO client
mc alias set myminio http://localhost:9000 minioadmin minioadmin

# Create bucket
mc mb myminio/documenso
```

### 3. Configure Environment Variables

```bash
NEXT_PUBLIC_UPLOAD_TRANSPORT=s3
NEXT_PRIVATE_UPLOAD_BUCKET=documenso
NEXT_PRIVATE_UPLOAD_ENDPOINT=http://minio:9000
NEXT_PRIVATE_UPLOAD_FORCE_PATH_STYLE=true
NEXT_PRIVATE_UPLOAD_ACCESS_KEY_ID=minioadmin
NEXT_PRIVATE_UPLOAD_SECRET_ACCESS_KEY=minioadmin
NEXT_PRIVATE_UPLOAD_REGION=us-east-1
```

<Callout type="info">
  Set `NEXT_PRIVATE_UPLOAD_FORCE_PATH_STYLE=true` for MinIO and other S3-compatible services that
  don't support virtual-hosted bucket URLs.
</Callout>

---

## Other S3-Compatible Services

Documenso works with any S3-compatible storage service. Configure the endpoint and enable path-style URLs if required.

### Cloudflare R2

```bash
NEXT_PUBLIC_UPLOAD_TRANSPORT=s3
NEXT_PRIVATE_UPLOAD_BUCKET=documenso
NEXT_PRIVATE_UPLOAD_ENDPOINT=https://<account-id>.r2.cloudflarestorage.com
NEXT_PRIVATE_UPLOAD_ACCESS_KEY_ID=your-r2-access-key
NEXT_PRIVATE_UPLOAD_SECRET_ACCESS_KEY=your-r2-secret-key
NEXT_PRIVATE_UPLOAD_REGION=auto
```

### DigitalOcean Spaces

```bash
NEXT_PUBLIC_UPLOAD_TRANSPORT=s3
NEXT_PRIVATE_UPLOAD_BUCKET=documenso
NEXT_PRIVATE_UPLOAD_ENDPOINT=https://nyc3.digitaloceanspaces.com
NEXT_PRIVATE_UPLOAD_ACCESS_KEY_ID=your-spaces-key
NEXT_PRIVATE_UPLOAD_SECRET_ACCESS_KEY=your-spaces-secret
NEXT_PRIVATE_UPLOAD_REGION=nyc3
```

### Backblaze B2

```bash
NEXT_PUBLIC_UPLOAD_TRANSPORT=s3
NEXT_PRIVATE_UPLOAD_BUCKET=documenso
NEXT_PRIVATE_UPLOAD_ENDPOINT=https://s3.us-west-004.backblazeb2.com
NEXT_PRIVATE_UPLOAD_ACCESS_KEY_ID=your-b2-key-id
NEXT_PRIVATE_UPLOAD_SECRET_ACCESS_KEY=your-b2-application-key
NEXT_PRIVATE_UPLOAD_REGION=us-west-004
```

### Wasabi

```bash
NEXT_PUBLIC_UPLOAD_TRANSPORT=s3
NEXT_PRIVATE_UPLOAD_BUCKET=documenso
NEXT_PRIVATE_UPLOAD_ENDPOINT=https://s3.us-east-1.wasabisys.com
NEXT_PRIVATE_UPLOAD_ACCESS_KEY_ID=your-wasabi-key
NEXT_PRIVATE_UPLOAD_SECRET_ACCESS_KEY=your-wasabi-secret
NEXT_PRIVATE_UPLOAD_REGION=us-east-1
```

---

## CloudFront CDN (Optional)

Use Amazon CloudFront to serve documents with lower latency and reduced S3 costs. CloudFront integration uses signed URLs for secure access.

### Prerequisites

- An S3 bucket configured for Documenso
- A CloudFront distribution with the S3 bucket as origin
- A CloudFront key pair for signing URLs

### 1. Create a CloudFront Distribution

1. Go to CloudFront in the AWS Console
2. Create a distribution with your S3 bucket as the origin
3. Configure Origin Access Control (OAC) to restrict direct S3 access
4. Set the default cache behavior to allow GET requests

### 2. Create a Key Pair

CloudFront signed URLs require a key pair:

1. Go to CloudFront > Key management > Public keys
2. Create a new public key
3. Create a key group containing the public key
4. Associate the key group with your distribution

Keep the private key secure - you'll need it for the environment variable.

### 3. Configure Environment Variables

```bash
# CloudFront distribution domain (without https://)
NEXT_PRIVATE_UPLOAD_DISTRIBUTION_DOMAIN=https://d1234567890.cloudfront.net

# CloudFront key pair ID
NEXT_PRIVATE_UPLOAD_DISTRIBUTION_KEY_ID=K1234567890ABC

# Private key contents (PEM format)
NEXT_PRIVATE_UPLOAD_DISTRIBUTION_KEY_CONTENTS="-----BEGIN RSA PRIVATE KEY-----
MIIEpAIBAAKCAQEA...
-----END RSA PRIVATE KEY-----"
```

<Callout type="warn">
  Store the private key securely. Use environment variables or secrets management rather than
  committing it to version control.
</Callout>

### How It Works

When CloudFront is configured:

1. File uploads still go directly to S3 via presigned URLs
2. File downloads use CloudFront signed URLs
3. CloudFront caches files at edge locations
4. Signed URLs expire after 1 hour

---

## Migration Between Storage Backends

Documenso does not provide automatic migration between storage backends. Each document's storage location is recorded in the database.

<Callout type="warn">
  Documents uploaded to one storage backend cannot be automatically migrated to another. Plan your
  storage strategy before deploying to production.
</Callout>

### Manual Migration Process

To migrate existing documents from database to S3 storage:

1. Export documents from the database
2. Upload them to S3
3. Update the `DocumentData` records to point to S3 paths

This requires custom scripts and database modifications. For production deployments, we recommend starting with S3 storage from the beginning.

### Hybrid Operation

During migration, Documenso can read from both backends. The `DocumentData.type` field indicates where each document is stored:

- `BYTES_64`: Stored in database
- `S3_PATH`: Stored in S3

New uploads use the configured `NEXT_PUBLIC_UPLOAD_TRANSPORT` backend.

---

## Storage Sizing

### Database Storage Estimates

When using database storage, plan for significant database growth:

| Documents/Month | Avg Size | Monthly Growth | Annual Growth |
| --------------- | -------- | -------------- | ------------- |
| 100             | 500KB    | ~50MB          | ~600MB        |
| 1,000           | 500KB    | ~500MB         | ~6GB          |
| 10,000          | 500KB    | ~5GB           | ~60GB         |

Database storage includes base64 encoding overhead (~33% increase).

### S3 Storage Estimates

S3 stores files without encoding overhead:

| Documents/Month | Avg Size | Monthly Growth | Annual Growth |
| --------------- | -------- | -------------- | ------------- |
| 100             | 500KB    | ~50MB          | ~600MB        |
| 1,000           | 500KB    | ~500MB         | ~6GB          |
| 10,000          | 500KB    | ~5GB           | ~60GB         |

### Cost Comparison

For high-volume deployments, S3 is more cost-effective:

| Aspect        | Database Storage             | S3 Storage              |
| ------------- | ---------------------------- | ----------------------- |
| Storage cost  | Database pricing (~$0.10/GB) | S3 pricing (~$0.023/GB) |
| Transfer cost | Database I/O                 | S3 requests + egress    |
| Backup cost   | Larger database backups      | Separate S3 backups     |
| Performance   | Degrades with size           | Consistent              |

---

## Upload Size Limits

Configure the maximum upload size displayed to users:

```bash
NEXT_PUBLIC_DOCUMENT_SIZE_UPLOAD_LIMIT=10
```

This value is in megabytes. The default is 5MB.

<Callout type="info">
  This environment variable controls the UI display. Actual limits may also be enforced by your
  reverse proxy, web server, or S3 configuration.
</Callout>

Ensure your infrastructure supports the configured limit:

- **Nginx**: Set `client_max_body_size`
- **S3**: Default object size limit is 5GB (multipart upload may be required for large files)
- **CloudFront**: Default limit is 50MB per request

---

## Troubleshooting

### Access Denied when uploading

```
Error: Access Denied
```

**Causes:**

- Incorrect IAM credentials
- Bucket policy doesn't allow the required operations
- CORS not configured for presigned URL uploads

**Solutions:**

1. Verify IAM user has `s3:PutObject` permission
2. Check bucket policy allows writes
3. Configure CORS to allow your domain

### Invalid endpoint

```
Error: getaddrinfo ENOTFOUND your-bucket.s3.amazonaws.com
```

**Causes:**

- Incorrect region configuration
- Bucket doesn't exist
- Network connectivity issues

**Solutions:**

1. Verify the bucket exists in the specified region
2. Check the endpoint URL is correct
3. Verify network access to S3

### SignatureDoesNotMatch

```
Error: The request signature we calculated does not match the signature you provided
```

**Causes:**

- Incorrect secret access key
- Clock skew between server and S3

**Solutions:**

1. Verify credentials are correct and not expired
2. Ensure server time is synchronized (use NTP)

### Path-style access required

```
Error: The bucket you are attempting to access must be addressed using the specified endpoint
```

**Solution:**

Set path-style access for S3-compatible services:

```bash
NEXT_PRIVATE_UPLOAD_FORCE_PATH_STYLE=true
```

### CloudFront signed URL errors

```
Error: Missing required key 'Key' in params
```

**Causes:**

- Key pair not configured correctly
- Private key format invalid

**Solutions:**

1. Verify the key pair ID matches the CloudFront configuration
2. Ensure the private key is in PEM format
3. Check for whitespace or encoding issues in the key contents

---

## See Also

- [Database Configuration](/docs/self-hosting/configuration/database) - Configure PostgreSQL
- [Environment Variables](/docs/self-hosting/configuration/environment) - Complete configuration reference
- [Backups](/docs/self-hosting/maintenance/backups) - Backup strategies for both storage backends
- [Docker Compose](/docs/self-hosting/deployment/docker-compose) - Deploy with MinIO for local storage
